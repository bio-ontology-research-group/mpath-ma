import pandas as pd
import os
import subprocess
import shutil
import re
import numpy as np
import glob
import argparse
import multiprocessing
from functools import partial

# Constants
FUNC_BIN_DEFAULT = "func_bin/func_hyper"


def setup_func_bin():
    # Locate func binary
    bin_path = os.path.abspath(FUNC_BIN_DEFAULT)
    if not shutil.which(bin_path):
        # Fallback locations
        if os.path.exists("func-0.4.10/src/hypergeometric/func_hyper"):
            bin_path = os.path.abspath("func-0.4.10/src/hypergeometric/func_hyper")
        elif os.path.exists("/usr/local/bin/func_hyper"):
            bin_path = "/usr/local/bin/func_hyper"
        else:
            return None
    return bin_path


def clean_iri(x):
    s = str(x)
    s = s.replace("http://purl.obolibrary.org/obo/", "")
    s = s.replace(">", "")
    if "/" in s:
        s = s.split("/")[-1]
    if "#" in s:
        s = s.split("#")[-1]
    return s


def clean_ma(x):
    return str(x).replace("MA:", "MA_")


def clean_mpath(x):
    return str(x).replace("MPATH:", "MPATH_")


def calculate_fdr_bh(p_values):
    p_values = np.array(p_values)
    n = len(p_values)
    if n == 0:
        return p_values

    sorted_indices = np.argsort(p_values)
    sorted_p = p_values[sorted_indices]

    ranks = np.arange(1, n + 1)
    fdr = sorted_p * n / ranks
    fdr = np.minimum(fdr, 1.0)

    # Enforce monotonicity
    for i in range(n - 2, -1, -1):
        fdr[i] = min(fdr[i], fdr[i + 1])

    original_fdr = np.zeros(n)
    original_fdr[sorted_indices] = fdr
    return original_fdr


def process_strain_analysis(args):
    # Unpack arguments
    (
        target_strain,
        analysis_info,
        animals,
        output_base_dir,
        func_bin,
        random_sets,
        cutoff,
        p_cutoff,
    ) = args

    aname = analysis_info["name"]
    adir = analysis_info["func_dir"]
    aroot = analysis_info["root"]

    # Identify Target Animals
    target_ids = [m for m, d in animals.items() if d["Strain"] == target_strain]
    if not target_ids:
        return []

    # Identify Background
    target_strata = set((animals[m]["Sex"], animals[m]["Code"]) for m in target_ids)
    background_ids = []
    for m, d in animals.items():
        if d["Strain"] == target_strain:
            continue
        if (d["Sex"], d["Code"]) in target_strata:
            background_ids.append(m)

    if not background_ids:
        return []

    # Prepare Input
    input_lines = []
    for m in target_ids:
        for t in animals[m]["Terms"]:
            input_lines.append(f"{m}\t{t}\t1")
    for m in background_ids:
        for t in animals[m]["Terms"]:
            input_lines.append(f"{m}\t{t}\t0")

    # File Paths
    safe_strain = re.sub(r"[^A-Za-z0-9]+", "_", target_strain)
    job_name = f"{aname}_{safe_strain}"
    job_dir = os.path.join(output_base_dir, job_name)
    if not os.path.exists(job_dir):
        os.makedirs(job_dir)

    input_file = os.path.join(job_dir, "input.txt")
    output_dir = os.path.join(job_dir, "output")

    with open(input_file, "w") as f:
        f.write("\n".join(input_lines))

    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    # Run func
    cmd = [
        func_bin,
        "-i",
        input_file,
        "-t",
        adir,
        "-o",
        output_dir,
        "-c",
        str(cutoff),
        "-g",
        aroot,
        "-r",
        str(random_sets),
    ]

    try:
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except subprocess.CalledProcessError as e:
        # print(f"Func failed for {job_name}: {e}")
        return []

    # Run Refinement
    # Refinement script is generated by func inside output_dir
    refin_scripts = glob.glob(os.path.join(output_dir, "refin-*.sh"))
    if refin_scripts:
        refin_script = refin_scripts[0]
        # Parameters: pvalue, pvalue-after-refinement, cutoff
        # Use p_cutoff for refinement threshold
        refin_cmd = [refin_script, str(p_cutoff), str(p_cutoff), str(cutoff)]
        try:
            subprocess.run(
                refin_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE
            )
        except subprocess.CalledProcessError:
            pass  # Refinement might fail if no results

    # Parse Results
    results = []

    # 1. Parse groups.txt for FWER/Original stats
    groups_file = os.path.join(output_dir, "groups.txt")
    groups_data = {}

    if os.path.exists(groups_file):
        try:
            # Manual parsing to handle -1 logic
            with open(groups_file, "r") as f:
                lines = f.readlines()

            # Header: root_node_name node_name node_id #genes_root #genes_node #hits_root #hits_node raw_p_u raw_p_o FWER_u FWER_o FDR_u FDR_o
            # Indices (0-based splitting by tab)
            # 0: root_name, 1: name, 2: id, 3: N, 4: K, 5: n, 6: k
            # 7: raw_u, 8: raw_o, 9: FWER_u, 10: FWER_o, 11: FDR_u, 12: FDR_o

            for line in lines[1:]:
                p = line.strip().split("\t")
                if len(p) < 13:
                    continue

                nid = p[2]

                # Helper to handle -1
                def get_val(idx, fallback_idx):
                    v = float(p[idx])
                    if v == -1 and fallback_idx is not None:
                        return float(p[fallback_idx])
                    return v

                # Parse Stats
                # Note: If FWER is -1, use Raw. If FDR is -1, use FWER.
                # Distance is 2 columns back.

                raw_u = float(p[7])
                raw_o = float(p[8])

                fwer_u = get_val(9, 7)
                fwer_o = get_val(10, 8)

                fdr_u = get_val(11, 9)
                fdr_o = get_val(12, 10)

                groups_data[nid] = {
                    "N": int(p[3]),
                    "K": int(p[4]),
                    "n": int(p[5]),
                    "k": int(p[6]),
                    "FWER_Over": fwer_o,
                    "FWER_Under": fwer_u,
                    "FDR_Over": fdr_o,
                    "FDR_Under": fdr_u,
                }

        except Exception:
            pass

    # 2. Parse Refinement Output
    refin_files = glob.glob(os.path.join(output_dir, "refinement-*.txt"))
    for rfile in refin_files:
        try:
            with open(rfile, "r") as f:
                lines = f.readlines()
            if len(lines) < 2:
                continue

            for line in lines[1:]:
                p = line.strip().split("\t")
                if len(p) < 10:
                    continue

                # 0: root, 1: name, 2: id, 3: sign
                # 4: hits(k), 5: total(K) -- Wait, let's verify indices from previous step or file content
                # "raw_p_under... raw_p_over... p_under_refin... p_over_refin"
                # usually indices 6, 7, 8, 9

                term_id = p[2]
                term_name = p[1]

                k = int(p[4])  # Genes in node?
                # Wait, "raw_p_underrepresentation_of_variable=1"
                # Before that: #genes_with_variable=1_in_root_node (n) ??
                # Let's check header in `func_hyper` script:
                # root_node_name node_name node_id sign? raw_p_u raw_p_o p_u_refin p_o_refin
                # It does NOT seem to have counts in the refinement file header in the script I read!
                # Script: echo -e "root_node_name\tnode_name\tnode_id\tsign?\traw_p_under...\traw_p_over...\tp_under_refin\tp_over_refin"
                # BUT then it calls `func_category_groups.pl` to append data.
                # `func_category_groups.pl` output format might include counts?
                # In `groups.txt`, it included counts.
                # If `refinement-*.txt` has counts, good. If not, use `groups_data`.
                # Let's assume `groups_data` is the source of truth for counts.

                if term_id not in groups_data:
                    continue
                stats = groups_data[term_id]

                # Parse P-values
                # Indices in refinement file (assuming `func_category_groups.pl` adds columns or matches header?)
                # Actually `func_category_groups.pl` usually formats the output.
                # Let's rely on relative positions from end?
                # Or use the `groups_data` raw p-values and ONLY read refined p-values from here?
                # But we need to know WHICH column is which.
                # If the header lines up...
                # 0: root, 1: name, 2: id, 3: sign,
                # 4: raw_u, 5: raw_o, 6: p_u_refin, 7: p_o_refin
                # Wait, the script said:
                # echo -e "... raw_p_u \t raw_p_o \t p_u_refin \t p_o_refin"
                # So indices 4,5,6,7.
                # BUT `func_category_groups.pl` usually outputs: N, K, n, k ...
                # If `func_category_groups.pl` is used, the file content might be different from the echo header!
                # The script does: `echo header > file`, then `func_category_groups.pl ... >> file`.
                # So the header applies to the content.
                # If `func_category_groups.pl` matches that header, then it does NOT output counts.
                # Let's check `func_category_groups.pl` behavior?
                # Or just assume the `groups.txt` counts are correct and use them.

                # Raw p-values in refinement file
                # raw_u = float(p[4])
                # raw_o = float(p[5])

                # Refined p-values
                # Handle -1: "same as 2 columns left"

                def get_p(idx):
                    val = float(p[idx])
                    if val == -1:
                        return float(p[idx - 2])
                    return val

                # Check line length to be sure
                # If length is 8 (0-7), then 6 is u_refin, 7 is o_refin.
                if len(p) >= 8:
                    pu_refin = get_p(6)
                    po_refin = get_p(7)
                else:
                    continue

                # Effect Size
                N = stats["N"]
                K = stats["K"]
                n = stats["n"]
                k = stats["k"]

                if k < cutoff:
                    continue

                ef = 0.0
                if n > 0 and K > 0:
                    ef = (k * N) / (n * K)

                # Store
                if po_refin <= 1.0:
                    results.append(
                        {
                            "Analysis": aname,
                            "Strain": target_strain,
                            "TermID": term_id,
                            "TermName": term_name,
                            "Direction": "Over",
                            "Hits": k,
                            "Total": K,
                            "P_Refin": po_refin,
                            "EffectSize": ef,
                            "FWER_Orig": stats["FWER_Over"],
                        }
                    )

                if pu_refin <= 1.0:
                    results.append(
                        {
                            "Analysis": aname,
                            "Strain": target_strain,
                            "TermID": term_id,
                            "TermName": term_name,
                            "Direction": "Under",
                            "Hits": k,
                            "Total": K,
                            "P_Refin": pu_refin,
                            "EffectSize": ef,
                            "FWER_Orig": stats["FWER_Under"],
                        }
                    )

        except Exception:
            pass

    return results


def main():
    parser = argparse.ArgumentParser(
        description="Ontology Overrepresentation Analysis using FUNC"
    )
    parser.add_argument(
        "--data", default="data/cleaned_with_pamt.tsv", help="Input TSV file"
    )
    parser.add_argument(
        "--output", default="func_results_final", help="Output directory"
    )
    parser.add_argument(
        "--random-sets",
        "-r",
        type=int,
        default=10000,
        help="Number of random sets for FWER (Default: 10000)",
    )
    parser.add_argument(
        "--cutoff",
        "-c",
        type=int,
        default=3,
        help="Minimum hits per group (Default: 3)",
    )
    parser.add_argument(
        "--p-value",
        "-p",
        type=float,
        default=0.05,
        help="P-value cutoff for reporting (Default: 0.05)",
    )
    parser.add_argument(
        "--workers",
        "-w",
        type=int,
        default=max(1, multiprocessing.cpu_count() - 1),
        help="Number of parallel workers",
    )

    args = parser.parse_args()

    func_bin = setup_func_bin()
    if not func_bin:
        print("Error: func_hyper binary not found.")
        return

    print(f"Using func binary: {func_bin}")
    print(f"Input: {args.data}")
    print(f"Output: {args.output}")
    print(f"Random Sets: {args.random_sets}")
    print(f"Cutoff: {args.cutoff}")
    print(f"Workers: {args.workers}")

    # Read Data
    df = pd.read_csv(args.data, sep="\t")
    df.columns = [c.strip() for c in df.columns]

    # Analyses Definitions
    analyses = [
        {
            "name": "PAMT",
            "col": "PAMT_IRI",
            "func_dir": "func_data_pamt",
            "root": "MPATH_0",
            "cleaner": clean_iri,
        },
        {
            "name": "MA",
            "col": "Organ MA code",
            "func_dir": "func_data_ma",
            "root": "owl:Thing",
            "cleaner": clean_ma,
        },
        {
            "name": "MPATH",
            "col": "Diagnoses MPATH code",
            "func_dir": "func_data_mpath",
            "root": "MPATH_0",
            "cleaner": clean_mpath,
        },
    ]

    all_results = []

    # Prepare Data per Analysis
    tasks = []

    for analysis in analyses:
        print(f"Preparing {analysis['name']}...")
        acol = analysis["col"]
        aclean = analysis["cleaner"]

        animals = {}
        for idx, row in df.iterrows():
            mid = str(row["new IDs"])
            strain = str(row["Strain"])
            sex = str(row["Sex"])
            code = str(row["Code"])
            val = row[acol]

            if pd.isna(val) or str(val) == "nan" or str(val).strip() == "":
                continue

            term = aclean(val)
            if mid not in animals:
                animals[mid] = {
                    "Strain": strain,
                    "Sex": sex,
                    "Code": code,
                    "Terms": set(),
                }
            animals[mid]["Terms"].add(term)

        all_strains = sorted(list(set(a["Strain"] for a in animals.values())))

        for strain in all_strains:
            tasks.append(
                (
                    strain,
                    analysis,
                    animals,
                    args.output,
                    func_bin,
                    args.random_sets,
                    args.cutoff,
                    args.p_value,
                )
            )

    print(f"Total tasks: {len(tasks)}")
    print("Running analyses...")

    with multiprocessing.Pool(args.workers) as pool:
        results_nested = pool.map(process_strain_analysis, tasks)

    for res in results_nested:
        all_results.extend(res)

    # Final Processing
    if all_results:
        final_df = pd.DataFrame(all_results)

        # Calculate FDR per (Analysis, Strain, Direction)
        final_df["FDR_Refin"] = 1.0
        for (ana, strn, direc), group in final_df.groupby(
            ["Analysis", "Strain", "Direction"]
        ):
            pvals = group["P_Refin"].values
            fdrs = calculate_fdr_bh(pvals)
            final_df.loc[group.index, "FDR_Refin"] = fdrs

        # Filter
        sig_df = final_df[final_df["P_Refin"] < args.p_value].copy()
        sig_df = sig_df.sort_values(["Analysis", "Strain", "Direction", "P_Refin"])

        outfile = "enrichment_results_final.tsv"
        sig_df.to_csv(outfile, sep="\t", index=False)
        print(f"\nAnalysis Complete. Results saved to {outfile}")
        print(sig_df.head(20).to_string(index=False))
    else:
        print("No significant results found.")


if __name__ == "__main__":
    main()
